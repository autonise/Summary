# 1711.00541 - TASNET: TIME-DOMAIN AUDIO SEPARATION NETWORK FOR REAL-TIME, SINGLE-CHANNEL SPEECH SEPARATION

## Short Terms used -

    FT - Fourier Transform
    
    CTC - Connectionist Temporal Classification

    STFT - Short Time Fourier Transform - https://www.google.com/url?q=https://en.wikipedia.org/wiki/Short-time_Fourier_transform&sa=D&ust=1557659103661000&usg=AFQjCNFckuV0ByEcj4dkyXGSKGlxXJjI3w
    ISTFT - Inverse Time Fourier Transform
    
    PIT - Permutation Independent Training - https://arxiv.org/pdf/1607.00325.pdf
    *Short Note on PIT* On the lines of CTC loss. But in CTC loss we minimise over all possible combinations. Here we 
    minimise over the combination which yields least loss over all possible combinations.
    
    LSTM - Long Short Term Memory
    
    SI-SNR - Scale Invariant - Signal to Noise Ratio -> 10*log(cot^2(theta)) theta is the angle between the two vectors
       
    SDR - Signal to Distortion Ratio - 10*log*(||actual||^2/(||error_interference + error_noise + error_artif||)^2)
    
## Summary - 

    
## ABSTRACT - 

### Limitations of Fourier Transform in speech - 

	FT might not be optimum representation
	
	Have to have longer time window for suficient frequency resolution in STFT
	
	Phase and Magnitude are decoupled in STFT, which seems to be a problem to the author. 
	DOUBT - Why?(Phase can't be worked upon due to noisy structure)

### Proposal - 
	
	Directly encoder-decoder structure with smaller latency in time domain. Used Non-negative encoder outputs.

### Advantages -
	
	Faster as no STFT or ISTFT required.
	
	Low-power, real-time implementation achieveable
	
## INTRODUCTION - 

    "Because of the difficulty in modifying the phase" DOUBT - Why? Because phase keeps changing(Noisy)
    
    Encoder part of the model is learnt using an autoencoder with 1-D Convolution and 1-D Deconvolution. DOUBT - Autoencoders were a 
    failiure so why use them? - Maybe they didn't use this to initialise just mentioned that this is similar to that.
    
## MODEL DESCRIPTION - 

### Problem Formulation - 
    
    Short Fixed Window of time is taken and decomposed using matrix factorisation. The collective decomposition of every
    speaker should yield the sound track. So Summation over Di*B = X
    
### Network design -

    Find Wk(1*N) using 1-D convolution. Use 4 layer deep uni-directional LSTM layers to predict Mik(1*N). Let B(N*L) be a matrix. Then minimise 
    loss by summing over all k(segments) and i(sources) D(Sik, np.matmul(np.multiply(Wk, Mik), B)), where D is some distance
    meteric like cross-entropy or MSE or cosine loss.
    
    W and B are learned in a non-negative fasion as in a non-negative autoencoder. B here acts as a deconv layer.
    
    The input signal is normalised so that every segment has unit variance and mean = 0
    
    Sik is similarly normalised.
    
    While testing the output is scaled back to get the source- i
    
    D(A(1*n), B(1*n)) = 10log(cot^2(theta)) where cos(theta) = <A, B>/(|A|*|B|)
    
    Now the Mik predicted should correspond to the correct source. How do we take care of that? - PIT
    
## Experiments - 

### Data-sets - 
    
    Two-speaker speech separation problem using WSJ0-2mix dataset.
    To reduce the computation cost the waveforms were down-sampled to 8 kHz.
    
### Network Configuration - 

    Clearly mentioned in the paper.
    Curriculum Training Strategy - Showing the model samples in a way which will speed-up or improve convergence.
    This paper - "We start the training the network on 0.5 second long utterances, and continue training on 4 second long utterances afterward."
    This paper - "We start the training the network on 0.5 second long utterances, and continue training on 4 second long utterances afterward."
    
### Evaluation 

    Using SI-SNR and SDR
    
### Results and Analysis - 

    Maybe Low-frequency signals are important for Speech separation as the basis learnt have peak frequency mostly below
    1kHz
    
## Conclusion

    "Experiments showed that our system was 6 times faster compared to the state-of-art STFT-based systems," Are just
    comparing on the basis of window-size used for STFT and TasNet. Not comparing the time taken by the model, in short,
    the complexity of the model. DOUBT - should be investigated? No. We are just doing research.
