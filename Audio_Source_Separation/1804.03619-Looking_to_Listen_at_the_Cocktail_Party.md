# 1804.03619-Looking_to_Listen_at_the_Cocktail_Party

## Summary

    Not a structured paper.

    The main idea is that

    1) We detect faces(Should be visible throughout the video
    2) We create features using the faces (Feature dim = 1024) and stack them together to create some sort of image
    3) We use convolution with bi-linear interpolation to get an output which has the height same as the output from the time-varying STFT.
    4) We use convolution on the STFT too and concatenate the facial features in time with the voice features.
    5) We use B-LSTM in time and then a FC layer to predict masks for the speakers which is multiplied with the STFT and inversed to get the voice of independent speakers
    6) Models have to be trained specifically depending upon the number of people

    Scope of improvement - Having a model which can work with variable number of faces and the model having a very small latency

## Abstract

    Using just Audio creates the Permutation Problem while using video it can be elevated, the paper claims to do this.
    They introduce AVSpeech having both video and audio.
    Their model is speaker indeendent.
    They have not done enough research as there are papers in the recent which claim speaker independent and give better results than them which they have claimed isn't so.

## Introduction

    Talk about label permutation problem
    They claim that using video they get superior results than using just audio
    They have defined the problem in general. No Clear information given about their approach.

## Related Work

    Speech Separation
    Read - Wang and Chen[2017] for a comprehensive view of recent audio-onl methods based on deep learning
    Methods to solve Permutation Problem -
        Deep Clustering - Hershey et al.[2016]
        Permutation Invariant Loss - Isik et al.[2016[ and Yu et al.[2017]
    Augment the data to remove noise by using the same speaker's voice from different timeline so that model focuses on lips rather than just voice.
    This will remove noise in a better way as people can have similar voice but we still distinguish them based on the lips.
    The above idea was used iby Gabbay et al.[2017]
    Audio Visual Datasets -

        CUAVE dataset - 36 subjects saying each digit from 0 to 9 five times with a total of 180 examples per digit.
        Mandarin sentences dataset - 320 utterances of Mandarin sentences, each sentence containing 10 Chinese characters with equally distributed phonemes.
        TDC-TIMIT dataset - 60 volunteer speakers with around 200 videos each. Every speaker recite various sentences from the TIMIT dataset and are recorded using both front-acing and 30-degree cameras.

    This paper evaluates their results on the above 3 datasets

    LRS dataset is not publicly available

## AVSpeech Dataset

    No interfering background signals
    The segments are of barying length between 3 and 10 seconds long
    Each clip has only one visible face and audible sound
    Contains Roughly 4700 hours of vieo segments
    Approximately 150,000 distinct speakers
    Dataset procurred from Youtube videos of lectures(ex. TED talks) and how-to videos
    ToDO - clearly mention how they created the dataset

## Audio-Visual Speech Separation Model

### Video and Audio Representation

    Use Google Cloud Vision API for face detector
    Pretrained face recognition model to extract one face embedding per frame - 1024 features
    The features are extracted from the lowest spatially invariant layer
    Using these features it is possible to extract facial expressions - Rudd et al.[2016]
    They experimented with raw pixels of faces but it did not lead to improved performance

    Pre-Processing -

        STFT of 3-second audio segments using both real and imaginary parts
        Power law compression to prevent loud audio from overwhelming soft audio
        Same processing is applied to both the noisy signal and clean reference signal

        At inference time the model can be applied to arbitrarily long segments of video.

    Output - A multiplicative spectrogram mask (Ratio Mask(RM) and Complex Ratio Masks(cRM))
    Ideal ratio mask is defined as the ratio between the magnitudes of the clean and noisy spectrogram and is assumed to lie between 0 and 1.
    Complex Ideal ratio mask is defined as the ratio of the complex clean and noisy spectrogram.
    cRM has a real component and an imaginary component.
    Real and Imaginary parts of the complex mask typically lie between -1 and 1, but they have used sigmoid compression to bound these complex mask value between 0 and 1 with reference to Wang et al.[2016]

    In RM only the magnitude mask is predicted and multiplied and the original noisy phase is used
    In cRM complex multiplication of the predicted cRM and noisy spectrogram.

    The model outputs speakers + 1 masks. +1 for background noise

    cRM works better than RM

### Network Architecture

    Parameters specified in Table 1

    Spectrogram sampling rate is 100Hz and the video sampling rate is 25Hz which is upsampled to 100Hz using simple nearest neighbour interpolation

    AV fusion - Concatenating

    Multiple Speakers - Dedicated Model for number of faces. Will have to train a different model for each case. And for cases where dedicated number is not available train a model to give output corresponding the face and all others as background.
    Visual features share same same weights across convolutional layers.

    A model which takes a single visual stream as input can be used in the general case in which either the number of speakers is unknowns or a dedicated multi-speaker model is unavailable.

### Implementation Details

    ReLU activation is used in all layers except last layer where Sigmoid is used.
    Batchsize=6
    Adam optimizer - 5,000,000 Gradient Steps
    Learning Rate 3.0e-5 which is reduced by half every 1.8 million steps
    Audio is resampled to 16KHz, stereo is converted to mono by only taking the left channel
    STFT is computed using Hann window of length 25ms and hop length of 10ms and FFT size is 512
    So the output for a 3 second sample is 257(bins)*298(Time steps, should have been 300 but due to window left 2 cases left out)*2(Complex and Real)
    Power Law compression is performed with p=0.3(A^0.3, where A is the input/output audio spectrogram)
    Resample the face embeddings from all videos to 25 FPS before training and inference by either removing or replicating embeddings, to have a shape of 75 for 3 seconds.
    Face detection, alignment and quality assessment are done using the tools described by Cole et al.[2016].
    Missing frames, if found, then we use a vector of zeros.
    Non-speech background noise is obtained from AudioSet, a large scale dataset of manually-annotated segments from YouTube videos.
    Meteric used - SDR from BSS Eval Toolbox
    3-second non-overlapping segments from the varying-length segments in the dataset is generated.
    They generated 1.5 million synthetic mixtures for all the models and experiments. For each experiment 90% was taken to be the training dataset, 10 % was used as test-set.
    No Hyper-parameter tuning was done so no need for validation set. No early stopping was done!

## Experiments and Results

    ### Comparision with Audio-Only -

        There is available - "There are no publicly available state-of-the-art audio-only speech enhancement/separation systems and relatively few publicly available datasets.."
        CHiME-2 dataset - SDR 14.6
        Permutation invariant training is introduced to train the audio-only model

    ### Quantitative Analysis on Synthetic Mixtures

        One speaker + noise(1S + Noise) -

            Linear combination of unnormalized clean speech and AudioSet noise:
            Mixi = AVSj + 0.3*AudioSetk
            All types of models perform well because noise is in a different spectrum.
            SDR - 16 DB of audio-only model

        Two speaker (2S clean) -

            Mixi = AVSj + AVSk

            (i) Pass Visual Feature of only one person and get output corresponding to only that person.
                The other person's voice should go to background noise. Averaging the SDR results of this model gives an improvement of 1.3 dB over AO baseline

            (ii) Jointly passing the Visual Feature of both the speakers. SDR increases by 0.4 dB

        Two speaker + noise(2S + Noise) -

            Mixi = ACSj + AVSk + 0.3*AudioSet

            (i) Pass Visual Feature of only one person and get ouput corresponding to only that person.
                The other person's voice should go to background noise. Averaging the SSDR results of this model gives an improvement of 0.1 dB over AO baseline

            (ii) Jointly passing the Visual Feature of both the speakers and having 3 output channels(1 for background). SDR increases 0.5 dB

        Three speakers (3S Clean) -


            (i) Single Visual and all others as noise - 0.5 dB improvement over AO

            (ii) Two Visual stream configuration gives - 0.5 dB improvement over AO

            (iii) Three Visual stream configuration gives - 1.4 dB improvement over AO

        Same Gender separation -

            Break down results on how the model performs on same gender and different gender

    ### Real-World Speech Separation

        This model is not suited for real time application but post-taking the video

        In Noisy bar case the entire speech signal itself is suppressed. This has been noted to be common in masking-based approach and direct prediction of the transform might help.

        1S + noise does not give much improvement when using visual features because noise is easily separable in the frequency domain. No need for visual information.

    ### Comparision with Previous Work in Audio-Visual Speech Separation and Enhancement

        Measures used - PESQ, STOI and SDR from the BSS eval toolbox

        Used ASR to compare qualitatively

    ### Additional Analysis

        Ablation Studies -

            They tried early fusion(Not exactly like CBN but concatenating without the Visual Network) 2 dB decrease was observed

        Bottleneck Features -

            If they squeezed the visual feature to one scalar per time step, it performs nearly the same (only 0.5 dB loss)

        Visualising the heatmap in the video -

            The lip and cheek portions were found to be highlighted showing that visual features is able to encode this information.

        Effect of missing visual information -

            It is observed that the model only concentrates on some frames to be able to choose which person is talking. Removing 2-3rd of the visual features just reduces 0.8 dB on average.


    ## Appendix

    ### A.1 SDR -

            SDR = 10*log10(||starget||^2/(||einterf + enoise + eartif||^2))

            Correlates well with the amount of noise present in the output, though it does not say about the quality of the voice

    ### A.2 ViSQOL -

            Look at the paper. Is a metric which is very close the quality of the voice which is judged by a human.
